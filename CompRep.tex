\documentclass[11pt]{article}
\usepackage{graphicx}
\usepackage{cite}
\author{S.~V.~Adams, S.~Cusworth, C.~M.~Maynard and others}
\title{Report on LFRic computational performance 2019}
\date{\today}


\begin{document}
\maketitle
\medskip
\section{Introduction\label{sec:intro}}
The LFRic infrastructure is designed to host a dynamical core (Gung Ho)
that is scalable to a very large degree across a distributed memory
computer. This is typically expressed over MPI. LFRic is also designed
to accommodate different programming models to target different
processor architectures. Currently, OpenMP 2.0 is supported for shared
memory parallelism on CPUs. This is being extended to support OpenACC
to target both shared memory parallelism and Instruction Level
Parallelism (IPL) on GPUs. Moreover, the infrastructure has been
extended to support parallel, asynchronous I/O using the XIOS 
client/server framework.
Other developments include the abstract solver API which allows for great
flexibility in constructing different solvers, redundant computation
algorithms into halo regions which boost the efficiency of shared
memory and reduce the amount of communication and finally a Multigrid
solver which will enable a large reduction in the cost of global communication.
Much of the model infrastructure is described in~\cite{LFRic}.

The LFRic model uses PSyclone to generate the Parallel System or PSy
layer. Here, data parallelism across the horizontal mesh is used,
exploiting both distributed and shared memory parallelism. This
exploits domain specific information, known to the science developer
and encoded as Metadata in the kernel layer. However, to
fully exploit the ILP on more parallel architectures such as GPUs, it
is necessary to extend PSyclone to fully parse individual Fortran
statements in the Kernel layer and then, for example, annotate the
source code with directives such as OpenACC or OpenMP. This work is just beginning,
however, the micro-benchmark~\cite{lfric-microbenchmarks} suite has been 
developed so compute intensive kernels can be used to explore what kernel
optimisation can be developed which can then be generated by PSyclone in the full
model. The PSyclone Kernel Extractor (PSKE) is being developed to
allow the automated extraction of kernels, or more specifically a
driver with a dump file containing the necessary data for the looping
over the horizontal mesh and degrees of freedom without any of the
LFRic infrastructure.

LFRic uses Fortran 2003 Object Orientation programming to support many of the
features described above. This is a very powerful programming
style which aids software development by allowing a clear separation
of concerns between different areas of the code, promoting code re-use
and code development by disentangling dependencies. However, the lack
of compiler support, or more precisely, the proliferation of compiler
bugs which prevent the use of particular compilers, either without specific
code work arounds or even at all, is major problem. Development has
been severely delayed whilst compiler bugs have been isolated and
reported and work-arounds sought. The lack of working Fortran 2003
compilers remains a source of concern and must be considered a risk to
the project.

The strategy to obtain compute performance for LFRic has four main
strands. I/O performance is achieved by using the XIOS capability for
asynchronous parallel I/O in order to hide the cost of writing data. 
Compute performance is achieved by enabling scalability, allowing 
many agents to do computation. This comes at the cost of communication. 
Local communication in the form of halo
exchange is reduced by employing a communication avoiding/reducing
algorithm of redundant computation into the halos and exploiting shared
memory threaded parallelism. Global communications are primarily in
the form of global sums required for iterative solvers. The global sum
is already vendor optimised both in the vendor supplied MPI library
implementation and the high-bandwidth, low-latency network of the
machine itself. The only way to reduce this cost and ultimate limit on
scalability is to use an algorithm with fewer global sums. This is the
Geometric multigrid pre-conditioner to the Helmholtz solver. The {\em
  on-node} compute performance of the LFRic code itself is examined by
considering the performance of computationally intensive kernels on
different processor architectures. The LFRic Microbenchmark suite can
be used to develop architecture specific optimisations.
The report is organised into sections on each of these four strands,
{\em viz.} I/O, local communication, global communication and
architecture specific optimisations.


\include{perf_section}

\include{io_section}

\bibliography{refs.bib}
\bibliographystyle{unsrt}

\end{document}

