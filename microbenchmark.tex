\section{CMA miniapp on Broadwell and TX2}

We generated a new microbenchmark using LFRic revision 15378.
Why the kernel was chosen.
columnwise\_op\_appinv\_kernel\_code() from gravity\_wave miniapp. At the trunk revision at the time the kernel was not being used in the primary gungho application.

The driver code was taken from the Psyclone-generated Psy-layer and the kernel was taken ``as is'' from the source code.
Global optimisations- colouring and OMP threading, parallelised over columns on the sphere

The number of vertical layers in the miniapp was modified from the default (10) to 256. While current NWP and climate GA and RA configurations use between 70 and 90 vertical levels we take 256 layers in order to increase the throughput of the benchmark and also in expectation that future configurations will run with increased vertical resolution.

The dinodump (/ PYSKE?) capability to generate expected output to which to compare against.


While generating the new microbenchmark a minor bug in kernel was discovered. Specifically the manner in which the kernel is called in gravity\_wave miniapp (and the supporting code comments) indicate that the kernel is called to update the target array but instead the kernel actually populates the array while overriding the initial contents. It was confirmed by the original author of the code that the functionality of the kernel was correct and the comments are in error. The bug has minimal performance impact on CPU architectures- an intent(inout) array should be intent(out)- but may have a larger impact on GPU architectures where the array would be being offloaded unnecessarily. 

Resultant microbenchmark can be found in the github repo.


It is worth noting that there are few opportunities for vectorisation in the current implementation.

We present timings measured using omp\_get\_wtime(). The number of iterations in the vertical has been increased from 1 to 1000. Run with -O3. Other compiler flags are available.


\subsection{Thread scaling}
Broadwell
Compilers used

\begin{itemize}
\item CCE 8.7.7
\item Intel 17.0.0.098
\item gnu 7.3.0
\end{itemize}

Here we use the latest stable compiler versions available on each platform at the time of writing.

On the Broadwell system used (Cray XC40) there is a larger selection of compiler versions available. During the development of this work we encountered notable differences in performance between compiler versions: for example, a measurable performance degradation between cce 8.7.2 and cce 8.7.7. The runtime on one OMP thread increases from 8.3s to 10.5s, in other words 8.7.2 is faster than ifort but 8.7.7 is slower than ifort. This discrepancy is outwith the observed run-to-run variability on the system and is unexplained at present. We use the more recent (but slightly less performant) 8.7.7 in order to allow a one-to-one comparison with the Isambard system which only has version 8.7.7 of CCE.


TX2
What is an Isambard?
\begin{itemize}
\item CCE 8.7.7
\item GNU 8.2.0
\item Allinea 19.0.0
\end{itemize}

Does TX2 doesn't have a turbo-limiter like Broadwell (need ref)

\subsection{CMA using drop-in library call}
Here we investigate whether the implementation of the Thomas algorithm as written in the kernel can be replaced with a call to a standard library- in this case LAPACK. The potential benefits of this include a potential improvement to runtimes of current architectures. But, more importantly calling a standard API allows us to tap into routines that have been optimised specifically for the architecture being used. This could provide performance portability while reducing porting and maintenance costs when moving between future compute platforms.
A potential problem with this design is that the memory layout used in columnwise operator type (check name of obj) objects differs from that expected by the LAPACK API.

The version of the microbenchmark used can be found in a branch of the github repo.

Thread scaling
Thread-scales OK but runtime is poor.
Is it memory copies? No, we tried taking these out but couldn't match ``vanilla'' performance.

\subsection{Varying the number of levels}
Taking the Cray compiler and changing the number of levels.
WORK NEEDED to find crossover point

\subsection{Discussion}
In avoiding building the LFRic infractruture we also avoid compiler support problems.
TX2 vs Broadwell
Single thread performance on Broadwell is comfortably better than TX2. However, when multithreading is considered then the machines are node-for-node comparable.
Use of drop-in lib
Use of LAPACK as a drop-in replacement for kernel. Not optimal at present- LAPACK is optimised for larger problems.

Worth investigating vectorised versions of the algorithm, especially in the context of GPUs.
